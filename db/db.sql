-- phpMyAdmin SQL Dump
-- version 4.7.7
-- https://www.phpmyadmin.net/
--
-- Хост: 127.0.0.1:3306
-- Время создания: Окт 18 2018 г., 19:33
-- Версия сервера: 5.7.20
-- Версия PHP: 7.2.0

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- База данных: `Blogs`
--

-- --------------------------------------------------------

--
-- Структура таблицы `comments`
--

CREATE TABLE `comments` (
  `id` int(11) NOT NULL,
  `post_id` int(11) DEFAULT NULL,
  `name` text,
  `email` text,
  `text` text
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Дамп данных таблицы `comments`
--

INSERT INTO `comments` (`id`, `post_id`, `name`, `email`, `text`) VALUES
(81, 23, 'mohak', 'mohak@gmail.com', 'We all know blog commenting is a good method for getting traffic and also get a dofollow links but many people forget about quality of a comment..why any one can approve your comment if this comment is not add any value to his reader…\r\nReally interesting post Harsh..'),
(82, 23, 'ishita rathi', 'ishita.rathi@gmail.com', 'Commenting 2-3 words is of no use. First always read complete blog. And yes commenting on others post is the best way of increasing traffic on your search engine i.e. backlinks.'),
(83, 23, 'Keri Bolton ', 'keri.bolton@gmail.com', 'I do follow your lines here that “Quality is better than quantity”. Yes, commenting can gain traffic for you but your comment needs to accomplish the requirements of the blog. Whatever your comment is, it should be relevant to the blog');

-- --------------------------------------------------------

--
-- Структура таблицы `posts`
--

CREATE TABLE `posts` (
  `id` int(11) NOT NULL,
  `title` text,
  `content` text
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Дамп данных таблицы `posts`
--

INSERT INTO `posts` (`id`, `title`, `content`) VALUES
(21, 'The Huffington Post', '<p>The history of political blogging might usefully be divided into the periods pre- and post-Huffington. Before the millionaire socialite Arianna Huffington decided to get in on the act, bloggers operated in a spirit of underdog solidarity. They hated the mainstream media - and the feeling was mutual. &nbsp; &nbsp; <a href=\"https://www.theguardian.com/info/2016/feb/15/sign-up-to-the-media-briefing\"></a></p><p>Bloggers saw themselves as gadflies, pricking the arrogance of established elites from their home computers, in their pyjamas, late into the night. So when, in 2005, Huffington decided to mobilise her fortune and media connections to create, from scratch, a flagship liberal blog she was roundly derided. Who, spluttered the original bloggerati, did she think she was?&nbsp;</p><p>But the pyjama purists were confounded. Arianna&#39;s money talked just as loudly online as off, and the Huffington Post quickly became one of the most influential and popular journals on the web. It recruited professional columnists and celebrity bloggers. It hoovered up traffic. Its launch was a landmark moment in the evolution of the web because it showed that many of the old rules still applied to the new medium: a bit of marketing savvy and deep pockets could go just as far as geek credibility, and get there faster.&nbsp;</p><p>To borrow the gold-rush simile beloved of web pioneers, Huffington&#39;s success made the first generation of bloggers look like two-bit prospectors panning for nuggets in shallow creeks before the big mining operations moved in. In the era pre-Huffington, big media companies ignored the web, or feared it; post-Huffington they started to treat it as just another marketplace, open to exploitation. Three years on, Rupert Murdoch owns MySpace, while newbie amateur bloggers have to gather traffic crumbs from under the table of the big-time publishers.&nbsp;</p><p><strong>Least likely to post</strong> &#39;I&#39;m so over this story - check out the New York Times&#39;</p><p><a data-link-name=\"in body link\" href=\"http://www.huffingtonpost.com\">huffingtonpost.com</a></p>'),
(22, 'Boing Boing', '<p>Lego reconstructions of pop videos and cakes baked in the shape of iPods are not generally considered relevant to serious political debate. But even the most earnest bloggers will often take time out of their busy schedule to pass on some titbit of mildly entertaining geek ephemera. No one has done more to promote pointless, yet strangely cool, time-wasting stuff on the net than the editors of Boing Boing (subtitle: A Directory of Wonderful Things). It launched in January 2000 and has had an immeasurable influence on the style and idiom of blogging. But hidden among the pictures of steam-powered CD players and Darth Vader tea towels there is a steely, ultra-liberal political agenda: championing the web as a global medium free of state and corporate control.&nbsp;</p><p>Boing Boing chronicles cases where despotic regimes have silenced or imprisoned bloggers. It helped channel blogger scorn on to Yahoo and Google when they kowtowed to China&#39;s censors in order to win investment opportunities. It was instrumental in exposing the creeping erosion of civil liberties in the US under post-9/11 &#39;Homeland Security&#39; legislation. And it routinely ridicules attempts by the music and film industries to persecute small-time file sharers and bedroom pirates instead of getting their own web strategies in order. It does it all with gentle, irreverent charm, polluted only occasionally with gratuitous smut.&nbsp;</p><p>Their dominance of the terrain where technology meets politics makes the Boing Boing crew geek aristocracy.&nbsp;</p><p><strong>Least likely to post</strong> &#39;Has anyone got a stamp?&#39;</p><p><a data-link-name=\"in body link\" href=\"http://www.boingboing.net\">boingboing.net</a>&nbsp;</p>'),
(23, 'Something is (still) rotten in the kingdom of artificial intelligence', '<h3 itemprop=\"description\">Four types of problems in the land of artificial intelligence</h3><div itemprop=\"articleBody\"><p>Nobody can deny that artificial intelligence (or machine learning, deep learning, or cognitive computing) is booming these days. And&mdash;as before, as this is in fact the second round for AI&mdash;the hype is almost unlimited. But there are serious problems, and I suspect it will not be long before they become undeniable again and we&rsquo;re back to a more realistic assessment of what the technology is bringing us.</p><p>There are roughly four types of problems in the land of AI. Let&rsquo;s start with an illustration of one of these, where the hype doesn&rsquo;t live up to reality yet and where it is quite possible that it never will.</p><h2>Gorillas</h2><p>In 2015, Google <a href=\"https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app\" rel=\"nofollow\">had to apologize</a> because its analytics software tagged a picture of two black people as gorillas. Google&rsquo;s chief architect at the time wrote: &ldquo;We used to have a problem with people (of all races) being tagged as dogs, for similar reasons. We&rsquo;re also working on longer-term fixes around both linguistics (words to be careful about in photos of people) and image recognition itself (e.g., better recognition of dark-skinned faces). Lots of work being done and lots still to be done, but we&rsquo;re very much on it.&rdquo; Fast-forward three years and it turns out what Google&rsquo;s quick and dirty fix was to: <a href=\"https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/\" rel=\"nofollow\">ban the word &ldquo;gorilla&rdquo; altogether</a>. This is very indicative of a fundamental problem: In three years&rsquo; time it was not able to fix the issue. Banning the word &ldquo;gorilla&rdquo; altogether is not a fix, it is an admission of failure.</p><aside><strong>[ Learn how to write apps that take full advantage of machine learning: <a href=\"https://www.infoworld.com/article/3269316/python/why-you-should-use-python-for-machine-learning.html\">Why you should use Python for machine learning</a>. &bull; <a href=\"https://www.infoworld.com/article/3198252/artificial-intelligence/data-in-intelligence-out-machine-learning-pipelines-demystified.html\">Data in, intelligence out: Machine learning pipelines demystified</a> &bull; <a href=\"https://www.infoworld.com/article/3197405/tpus-googles-machine-learning-pipeline-explained.html\">Google&rsquo;s machine-learning cloud pipeline explained</a> &bull; <a href=\"https://www.infoworld.com/article/3192177/sql-server-2017-makes-the-move-to-machine-learning.html\">R and Python drive SQL Server 2017 into machine learning</a>. | Keep up with hot topics in programming with InfoWorld&rsquo;s <a href=\"https://www.infoworld.com/newsletters/signup.html\">App Dev Report newsletter</a>. ]</strong></aside><p>And here is a sign that we&rsquo;re still deep in hype mode: when reporting on this, <a href=\"https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people\" rel=\"nofollow\">the <em>Guardian</em> writes</a>: &ldquo;The failure of the company to develop a more sustainable fix in the following two years highlights the extent to which machine learning technology, which underpins the image recognition feature, is <em>still maturing</em>&rdquo; (italics mine). This is a sign of hype as it assumes (without reasonable proof) that the problem is solvable <em>at all</em>. And it is by definition limited in time, as you cannot keep writing that something is &lsquo;maturing&rsquo; for extended periods of time.</p><p>These kinds of problems (and this kind of reporting in the press fully mirrors what happened in the 1960s and 1970s. Many reported successes weren&rsquo;t real and the failures were either painted over, not reported, or labelled &ldquo;still maturing,&rdquo; implying: there is nothing wrong with the approach itself. I already noticed the same vein in reporting in journals like the<em>&nbsp;New Scientist</em> already years ago. Good examples were articles listed &ldquo;the X most interesting scientific and technological breakthroughs.&rdquo; Such lists generally contained at least one AI-like item and such an item was often <em>also</em> the only item that wasn&rsquo;t reality yet. Often (just as in the first AI hype period) these were reported as failures &ldquo;that just needed to mature,&rdquo; &ldquo;fix a few outstanding issues,&rdquo; etc. All the others in the list were proven breakthroughs (e.g., a mechanism that worked, even in volume, but only was too expensive yet to produce in volume), but the AI ones were always only breakthroughs if we assumed totally unfounded extrapolations. There is a disturbing lack of criticism when AI is being reported upon&mdash;by the way, I like <em>New Scientist</em> a lot. This just shows that even serious science journalism is not immune to the hype.</p><p>For those experienced enough (a nice way of saying &ldquo;old buggers&rdquo;) to have been on the inside of the first period of AI hype (1960 to 1990), the current hype shows some distressing parallels with the previous one, in which hundreds of billions were spent in vain to get the promised &ldquo;artificial intelligence.&rdquo; In 1972, from Hubert Dreyfus&rsquo;s book <em>What Computers Can&rsquo;t Do</em>, it was already clear the approaches were doomed. It is a testament to the strength of (mistaken) conviction that was behind the first AI hype that it took another 20 years and billions of dollars, spent by governments (mainly in the US, the EU, and Japan) and private companies (Bill Gates&rsquo;s Microsoft was a big believer for instance) for the AI hype to peter out and the &ldquo;<a href=\"https://en.wikipedia.org/wiki/AI_winter\" rel=\"nofollow\">winter of AI</a>&rdquo; to set in.</p><aside>&nbsp;</aside><p>Did that mean AI failed completely in the previous century? Not completely, but what definitely did fail was the initial wave and the initial assumptions on which it was built. This current AI hype is seemingly built on different assumptions, so theoretically it might succeed. The question then becomes &ldquo;Are we closing in on the goal this time?&rdquo; The answer is again no, for some fundamental reasons I mention below, but the situation is also different and more actual value results from the current efforts.</p><aside><strong>[ <a href=\"https://pluralsight.pxf.io/c/321564/424552/7490?u=https%3A%2F%2Fwww.pluralsight.com%2Fcourses%2Funderstanding-enterprise-architecture\" rel=\"nofollow\">Enterprise architects lead digital transformations. This PluralSight intro course explains the fundamentals of EA and its latest practices.</a> ]</strong></aside><h2>Great expectations and failure, round 1</h2><p>With the early rise of computers in the 1950s and 1960s there followed a fast rise in the belief that these wonderful computers, that could calculate so much faster than any human could, would give rise to artificial (human-like) intelligence. Early pioneers wrote small scale programs that were able to first beat humans in tic-tac-toe, then checkers, and in 1970 we already read predictions that computers would be &ldquo;as smart as humans in a matter of years&rdquo; and &ldquo;much smarter a few months later.&rdquo; Within ten years the world chess champion would be beaten by a computer.</p><aside>&nbsp;</aside><p>Computers as intelligent as humans (even if specialist programs eventually could outcalculate human intelligence in microworlds such as chess) failed to materialize decade after decade, until in the 1980s the &ldquo;winter of AI&rdquo; slowly set in. I often lament the billions Bill Gates&rsquo;s Microsoft poured into AI (in which he believed strongly) to produce duds like Microsoft Bob, Microsoft Agent, the infamous Clippy, and lots of vaporware, where it could have fixed its software to be less of a security nightmare and actually work decently (e.g., it is moronic that even in Windows 10 <em>today</em>, if you share a spreadsheet from within Excel, which creates a mail message in Outlook, Outlook is incapable to do <em>anything</em> else until the window containing the spreadsheet to be sent has been closed. Something NeXTStep, now MacOS, could already do 25 years ago, but I digress).</p><p>But the whole subject in the end came to nothing. Expert systems turned out to be costly to build and maintain and very, very brittle. Machine language translation based on grammars (rules) and dictionaries never amounted to much. The actual successes often had one thing in common: they restricted the problem to solve to something computers could manage, a microworld. For instance, using a small subset of English (e.g., <a href=\"https://en.wikipedia.org/wiki/Simplified_Technical_English\" rel=\"nofollow\">Simplified Technical English</a>). Or, using the fact that the possible <em>existing</em> <em>combinations</em> of street names, numbers, postal codes, and city names gave such a good limitation of the search space that mail-sorting machines could get by with a measly 70 percent accuracy of actually reading the addresses themselves.</p><p>As mentioned above, the failure was already predicted in the late 1960s by Dreyfus, who&mdash;after studying the approaches and ideas of the cognitive science and AI communities&mdash;noticed that their approaches were based on misunderstandings already known from analytic philosophy, which had hit those same limitations before. But who listens to philosophers, even if they are of the analytic kind?</p><p>Anyway, in 1979, the second revised edition of Dreyfus&rsquo;s seminal book was published. It showed, that much of the additionally reported progress was again illusory and it analyzed why. And finally, in 1992 the third and definitive edition was published by MIT Press (MIT being a hotbed of anti-Dreyfus sentiment in the years before). Twenty years in a field supposedly developing with a speed beyond everything that went before, with computers that had become many orders of magnitude more powerful, but the book remained correct, and in fact is correct (and worthwhile to read) to this day.</p><aside>&nbsp;</aside><p>In the 1992 edition, Dreyfus described an early failure of the then-nascent &ldquo;neural networks&rdquo; approach (the approach that underlies many of today&rsquo;s successes, such as Google beating the human Go champion). The U.S. Department of Defense had commissioned a program that would&mdash;based on neural network technology&mdash;be able to detect tanks in a forest based on photos. The researchers built the program and fed it with the first batch of a set of photos that showed the same location, but one with a tank and one without. After having trained the neural network thus, the program was able to detect tanks with an uncanny precision on photos from the training batch itself. Even more impressive, when they used the rest of the set (the photos that had not been used to train the neural network), the program did extremely well and the Department of Defense was much impressed. Too soon, though. The researchers went back to the forest to take more pictures and were surprised to find that the neural network was not capable of discriminating between pictures with and without tanks&nbsp;at all.&nbsp;An investigation provided the answer: It took time to put tanks in place or remove them.</p><p>As a result, the original photos with tanks had been made on a cloudy day, the photos without tanks had been made on a sunny day. The network had effectively been trained to discriminate between a sunny (sharp shadows) and a cloudy day (hardly shadows). Fast-forward thirty (!) years and the deep learning neural networks or equivalents from Google run into <em>exactly</em> the same problem. Their neural networks have learned to discriminate, but not enough to discriminate between black people and gorillas, and it is actually not very clear <em>what</em> they discriminate on.</p><p>And yes, Google can now beat the best people at Go, which is much more difficult than chess (where IBM beat the best people in the 1990s), but Go and Chess <em>are</em> still examples of the microworlds of the late 1960s. Even more important: these are strictly <em>logical</em> (discrete) microworlds, and people are <em>extremely</em> bad at (discrete) logic. We might be better at it than any other living thing on this planet, but we&rsquo;re still much better at Frisbee. (This by the way is one of the reasons why rule-based approaches for enterprise/IT architecture fare so poorly: The world in which enterprises/computers must act is not logical at all.)</p><h2>Great expectations, round 2. Failure ahead?</h2><p>Neural networks and other forms of statistics require lots of data and &ldquo;deep&rdquo; (i.e., many-layered) networks to amount to anything. In the 1960s and 1970s the computers were puny and the data sets were tiny. So, generally, the &ldquo;intelligence&rdquo; was based on direct creation of symbolic rules in expert systems without a need for statistics on data. These rule-based systems (sometimes disguised as data-driven&mdash;such as the <a href=\"https://en.wikipedia.org/wiki/Cyc\" rel=\"nofollow\">Cyc</a> project which stubbornly plods along with the goal of modeling all facts and thus come to intelligence&mdash;failed outside of a few microworlds. Cyc, by the way, is proving that even with a million facts and rules, you still have nothing that looks like true intelligence.</p><p>Anyway, the first neural networks were &ldquo;thin,&rdquo; initially two, maybe three layers to create the correlations between input (e.g., photo) and output (&ldquo;Tank!&rdquo;). Such thin networks are very brittle. With more &ldquo;depth&rdquo; today, they have more hidden rules. The statistical correlations are still equivalent to rules, these days deeper than mistaking a sunny day versus a cloudy day for &ldquo;tank&rdquo; versus &ldquo;no tank.&rdquo; In fact, while we do not know the actual rules, the neural networks on digital computers are still <em>data-driven, rule-based systems in disguise</em>. Smart &ldquo;psychometrics&rdquo; systems <a href=\"https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump\" rel=\"nofollow\">may still contain relations like between liking &ldquo;I hate Israel&rdquo; on Facebook with a tendency to like Nike shoes and Kit Kat</a>.</p><p>Now, over the last decade, the amount of data that is available has become huge (or, at least according to our current view, don&rsquo;t forget that researchers in the 1960s already talked about the issues that came with trying to program the &ldquo;very powerful&rdquo; computers of the day). This has opened up a new resource&mdash;data, in this light sometimes called the new &ldquo;oil&rdquo;&mdash;that can be exploited with statistics. To us, <em>logically</em> woefully underpowered people, it looks like magic and intelligence. These statistical tricks can draw reliable conclusions that are impossible for us to draw, if alone because we as human beings are unable to process all that data.</p><p>But look a bit deeper, and you will still find shockingly simple rules that are in fact unearthed and used. One specialist told me: Most of the time, when we do statistical research on the data, we find correlations around some 14 different aspects: ZIP code, age, sex, income, education, etc. And these predict with a decent enough reliability how we should approach customers.</p><p>The patterns (rules) unearthed are thus rather simple and have two important properties:</p><ul><li>They are not perfectly reliable. They all come with a percentage of reliability, as in &ldquo;90 percent reliable correlation.&rdquo; That means in many cases they produce erroneous results.</li><li>The more restricted the range of predictions, the more reliable they are.</li></ul>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</div>');

--
-- Индексы сохранённых таблиц
--

--
-- Индексы таблицы `comments`
--
ALTER TABLE `comments`
  ADD PRIMARY KEY (`id`);

--
-- Индексы таблицы `posts`
--
ALTER TABLE `posts`
  ADD PRIMARY KEY (`id`);

--
-- AUTO_INCREMENT для сохранённых таблиц
--

--
-- AUTO_INCREMENT для таблицы `comments`
--
ALTER TABLE `comments`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=88;

--
-- AUTO_INCREMENT для таблицы `posts`
--
ALTER TABLE `posts`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=32;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
